# Implicit regularisation of (S)GD by using macroscopic stepsizes for Diagonal Linear Networks

*Nafissa BENALI & Ángel REYERO LOBO - February 2024*

Teacher: *Hédi HADIJI*
## **Context**
This project is undertaken as part of the evaluation for the *Theoretical Deep Learning* course of CentraleSupélec in the second year of the Master's program in *Mathematics and Artificial Intelligence* at the Institut Mathématique d'Orsay (IMO) at Paris-Saclay University.

## **Overview**
The objective of this study is to theoretically investigate why entering the Edge of Stability regime (the narrow window preceding the divergence threshold for step sizes) is advantageous for Stochastic Gradient Descent (SGD), whereas it is not beneficial for Gradient Descent (GD). 


## **Structure**
This repository comprises the presentation slides for the project, a detailed report outlining how to interpret the main result, and a Jupyter Notebook file containing the functions used to create the graphs featured in the previous files. 

## **How to run the code**
To download all the figures, it is necessary to create a *Project* folder in the same folder where the file is run. Additionally, we recommend reducing the number of simulations for faster results.

## **Bibliography**
 Even, M., Pesme, S., Gunasekar, S., and Flammarion, N. (2023). (s)gd over diagonal linear networks:
 Implicit regularisation, large stepsizes and edge of stability.

